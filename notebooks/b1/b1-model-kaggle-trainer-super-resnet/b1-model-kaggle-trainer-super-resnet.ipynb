{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2f11d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:27.038762Z",
     "iopub.status.busy": "2025-02-06T22:48:27.038444Z",
     "iopub.status.idle": "2025-02-06T22:48:34.902707Z",
     "shell.execute_reply": "2025-02-06T22:48:34.902004Z"
    },
    "papermill": {
     "duration": 7.871975,
     "end_time": "2025-02-06T22:48:34.904251",
     "exception": false,
     "start_time": "2025-02-06T22:48:27.032276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import os\n",
    "from torchvision.transforms import v2\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f0c81",
   "metadata": {
    "papermill": {
     "duration": 0.004202,
     "end_time": "2025-02-06T22:48:34.913206",
     "exception": false,
     "start_time": "2025-02-06T22:48:34.909004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591b925a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:34.922667Z",
     "iopub.status.busy": "2025-02-06T22:48:34.922236Z",
     "iopub.status.idle": "2025-02-06T22:48:34.927492Z",
     "shell.execute_reply": "2025-02-06T22:48:34.926663Z"
    },
    "papermill": {
     "duration": 0.011252,
     "end_time": "2025-02-06T22:48:34.928639",
     "exception": false,
     "start_time": "2025-02-06T22:48:34.917387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "\n",
    "    def plot_training_val_b1(self, training_loss, val_loss, val_accuracy):\n",
    "        epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.plot(epochs, training_loss, 'b-', label='Training Loss')\n",
    "        ax1.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, val_accuracy, 'g-', label='Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('Validation Accuracy')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "plotter = Plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006edb4",
   "metadata": {
    "papermill": {
     "duration": 0.003908,
     "end_time": "2025-02-06T22:48:34.936903",
     "exception": false,
     "start_time": "2025-02-06T22:48:34.932995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfc3f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:34.946123Z",
     "iopub.status.busy": "2025-02-06T22:48:34.945849Z",
     "iopub.status.idle": "2025-02-06T22:48:34.955413Z",
     "shell.execute_reply": "2025-02-06T22:48:34.954785Z"
    },
    "papermill": {
     "duration": 0.015579,
     "end_time": "2025-02-06T22:48:34.956581",
     "exception": false,
     "start_time": "2025-02-06T22:48:34.941002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AnnotationProcessor:\n",
    "    def __init__(self, base_path, output_path='/kaggle/working/', filename='dataset.csv'):\n",
    "        self.base_path = base_path\n",
    "        self.output_path = output_path\n",
    "        self.filename = filename\n",
    "        self.data = None\n",
    "        self.run()\n",
    "\n",
    "    def get_group_annotation(self, file, folder_name):\n",
    "        \"\"\"Extract the first two elements from each row of the annotation file.\"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            data = [line.split()[:2] for line in f]\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['FrameID', 'Label'])\n",
    "        df['video_names'] = folder_name\n",
    "## ['l-spike', 'l_set', 'r_set', 'r-pass', 'r_spike', 'l-pass',\n",
    "       #'r_winpoint', 'l_winpoint']\n",
    "        label_mapping = {'l-spike': 0, 'l_set': 1, 'r_set': 2, 'r-pass': 3, 'r_spike': 4, 'l-pass': 5, 'r_winpoint': 6, 'l_winpoint': 7}\n",
    "        df['Mapped_Label'] = df['Label'].map(label_mapping).astype('int64')\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "        # Save the file directly in the root of the output path\n",
    "        df.to_csv(os.path.join(self.output_path, f'{folder_name}.csv'), index=False)\n",
    "\n",
    "    def process_annotations(self):\n",
    "        \"\"\"Process annotations from all folders in the base path.\"\"\"\n",
    "        for folder_name in os.listdir(self.base_path):\n",
    "            folder_path = os.path.join(self.base_path, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                annotated_file_path = os.path.join(folder_path, 'annotations.txt')\n",
    "                self.get_group_annotation(annotated_file_path, folder_name)\n",
    "\n",
    "    def combine_csv_files(self):\n",
    "        \"\"\"Combine all CSV files into a single DataFrame.\"\"\"\n",
    "        csv_files = glob.glob(os.path.join(self.output_path, '*.csv'))\n",
    "        data = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "        for d in data:\n",
    "            d.dropna(inplace=True)\n",
    "            # print(d.isna().sum())\n",
    "        return pd.concat(data, ignore_index=True)\n",
    "\n",
    "    def generate_img_paths(self, df):\n",
    "        \"\"\"Generate image paths based on the DataFrame.\"\"\"\n",
    "        df['img_path'] = df.apply(\n",
    "            lambda x: os.path.join(\n",
    "                self.base_path,\n",
    "                str(x['video_names']),  # Ensure `video_names` is a string\n",
    "                str(x['FrameID'])[:-4],  # Ensure `FrameID` is a string and remove the last 4 characters\n",
    "                str(x['FrameID'])  # Ensure `FrameID` is a string\n",
    "            ), axis=1\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def save_combined_data(self, df):\n",
    "        \"\"\"Save the combined data to a CSV file.\"\"\"\n",
    "        # Ensure the output directory exists before saving\n",
    "        os.makedirs(self.output_path, exist_ok=True)\n",
    "        # Save the combined data directly in the root of the output path\n",
    "        df.to_csv(os.path.join(self.output_path, self.filename), index=False)\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Delete all intermediate CSV files except the final output file.\"\"\"\n",
    "        for csv_file in glob.glob(os.path.join(self.output_path, '*.csv')):\n",
    "            if not csv_file.endswith(self.filename):\n",
    "                os.remove(csv_file)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the whole annotation processing pipeline.\"\"\"\n",
    "        self.process_annotations()\n",
    "        combined_data = self.combine_csv_files()\n",
    "        data_with_paths = self.generate_img_paths(combined_data)\n",
    "        self.save_combined_data(data_with_paths)\n",
    "        self.cleanup()  # Clean up intermediate files\n",
    "        self.data = pd.read_csv(os.path.join(self.output_path, self.filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b97165",
   "metadata": {
    "papermill": {
     "duration": 0.003917,
     "end_time": "2025-02-06T22:48:34.964782",
     "exception": false,
     "start_time": "2025-02-06T22:48:34.960865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Make sure you want to run this before running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86034a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:34.974066Z",
     "iopub.status.busy": "2025-02-06T22:48:34.973822Z",
     "iopub.status.idle": "2025-02-06T22:48:35.102453Z",
     "shell.execute_reply": "2025-02-06T22:48:35.101357Z"
    },
    "papermill": {
     "duration": 0.135134,
     "end_time": "2025-02-06T22:48:35.104019",
     "exception": false,
     "start_time": "2025-02-06T22:48:34.968885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/': Device or resource busy\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37916f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:35.113719Z",
     "iopub.status.busy": "2025-02-06T22:48:35.113445Z",
     "iopub.status.idle": "2025-02-06T22:48:35.960320Z",
     "shell.execute_reply": "2025-02-06T22:48:35.959389Z"
    },
    "papermill": {
     "duration": 0.853356,
     "end_time": "2025-02-06T22:48:35.961862",
     "exception": false,
     "start_time": "2025-02-06T22:48:35.108506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/volleyball/volleyball_/videos/'\n",
    "df = AnnotationProcessor(base_path, 'dataset.csv').data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492d8a55",
   "metadata": {
    "papermill": {
     "duration": 0.004832,
     "end_time": "2025-02-06T22:48:35.973691",
     "exception": false,
     "start_time": "2025-02-06T22:48:35.968859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57fdd716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:35.983518Z",
     "iopub.status.busy": "2025-02-06T22:48:35.983269Z",
     "iopub.status.idle": "2025-02-06T22:48:35.988410Z",
     "shell.execute_reply": "2025-02-06T22:48:35.987803Z"
    },
    "papermill": {
     "duration": 0.011409,
     "end_time": "2025-02-06T22:48:35.989573",
     "exception": false,
     "start_time": "2025-02-06T22:48:35.978164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResnetEvolution(nn.Module):\n",
    "    def __init__(self, hidden_layers=[]):\n",
    "        super(ResnetEvolution, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.model = self.__init_backbone(torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2))\n",
    "\n",
    "    def __init_backbone(self, backbone):\n",
    "        num_features = backbone.fc.in_features\n",
    "\n",
    "        layers = []\n",
    "        input_size = num_features  # Start with backbone output size\n",
    "        for hidden_size in self.hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())  # Activation function\n",
    "            input_size = hidden_size  # Update input for next layer\n",
    "\n",
    "        layers.append(nn.Linear(input_size, 8))  # Final output layer\n",
    "\n",
    "        backbone.fc = nn.Sequential(*layers)  # Output layer for binary classification\n",
    "\n",
    "        return backbone\n",
    "\n",
    "    def get_fc(self):\n",
    "        return self.model.fc\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d1711",
   "metadata": {
    "papermill": {
     "duration": 0.004054,
     "end_time": "2025-02-06T22:48:35.997800",
     "exception": false,
     "start_time": "2025-02-06T22:48:35.993746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ccaf68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.006965Z",
     "iopub.status.busy": "2025-02-06T22:48:36.006719Z",
     "iopub.status.idle": "2025-02-06T22:48:36.023013Z",
     "shell.execute_reply": "2025-02-06T22:48:36.022435Z"
    },
    "papermill": {
     "duration": 0.022316,
     "end_time": "2025-02-06T22:48:36.024238",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.001922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class b1_ModelTrainer:\n",
    "    def __init__(self, model, optimizer, scheduled, criterion, epochs, dataloaders, device, save_folder,\n",
    "                 is_continue=False, checkpoint=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduled = scheduled\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        self.DEVICE = device\n",
    "        self.save_folder = save_folder\n",
    "        self.is_continue = is_continue\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    # verbose 1 : checkpoint,\n",
    "    # verbose 3:  labels, preds\n",
    "    # verbose 4: logits\n",
    "    def train_model(self, verbose=0):\n",
    "        model, optimizer, criterion, epochs, dataloaders = self.model, self.optimizer, self.criterion, self.epochs, self.dataloaders\n",
    "\n",
    "        epoch = 0\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        if self.is_continue:\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"Continuing from checkpoint {self.checkpoint}\")\n",
    "\n",
    "            epoch, model, optimizer = self.__load_checkpoint(model, optimizer, self.checkpoint, verbose)\n",
    "\n",
    "        for training_epoch in range(epoch, epochs):\n",
    "\n",
    "            print(f\"\\nTraining epoch {training_epoch+1}\")\n",
    "\n",
    "            ## change model mode depending on the phase\n",
    "            for phase in ['train', 'val']:\n",
    "                dataloader = dataloaders[phase]\n",
    "                epoch_loss = 0  # Track total loss for the epoch\n",
    "                if phase == 'train':\n",
    "                    if verbose > 0:\n",
    "                        dataloader = tqdm(dataloader, desc=phase)\n",
    "                    model.train()\n",
    "                    for inputs, labels in dataloader:\n",
    "\n",
    "                        inputs = inputs.to(self.DEVICE)\n",
    "                        labels = labels.to(self.DEVICE)\n",
    "\n",
    "                        if verbose > 3:\n",
    "                            print(f\"labels: {labels}\")\n",
    "\n",
    "                        # zero grads of he optim\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # freeze the non-learnable weights\n",
    "                        # self.__handle_transfer_learning(phase, training_epoch / epochs)\n",
    "\n",
    "                        # forward pass\n",
    "                        logit = model(inputs)\n",
    "\n",
    "                        if verbose > 3:\n",
    "                            print(f\"logit: {logit}\")\n",
    "\n",
    "                        loss = criterion(logit, labels)\n",
    "                        loss.backward()\n",
    "                        # update weights\n",
    "                        optimizer.step()\n",
    "                        epoch_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "                    train_losses.append(epoch_loss / len(dataloader))\n",
    "                    print(\n",
    "                        f\"Epoch {training_epoch + 1}/{epochs}, {phase} Loss: {epoch_loss / len(dataloader)}\")  # Print loss\n",
    "                else:\n",
    "                    # skip evaluation if no suitable dataloader\n",
    "                    if dataloaders[phase] is None:\n",
    "                        continue\n",
    "                    model.eval()\n",
    "                    loss, acc = self.__eval_model(dataloader, verbose)\n",
    "                    val_losses.append(loss)\n",
    "                    val_accuracies.append(acc)\n",
    "                    print(\n",
    "                        f\"Epoch {training_epoch + 1}/{epochs}, ({phase}) Loss: {loss} | Accuracy: {acc}\")  # Print loss\n",
    "\n",
    "            if self.scheduled:\n",
    "                optimizer.scheduler_step()\n",
    "                self.__save_checkpoint(training_epoch, model.state_dict(), optimizer.optimizer_state_dict(),\n",
    "                                       optimizer.scheduler_state_dict(), verbose)\n",
    "            else:\n",
    "                self.__save_checkpoint(training_epoch, model.state_dict(), optimizer.state_dict(), verbose)\n",
    "\n",
    "            if training_epoch % 10 == 0:\n",
    "                self.__save_model(training_epoch, verbose)\n",
    "\n",
    "        self.__save_model('final_', verbose)\n",
    "        return train_losses, val_losses, val_accuracies\n",
    "\n",
    "    def __handle_transfer_learning(self, phase, ratio_epochs, tl_coeff=0, verbose=0):\n",
    "        if phase == \"train\":\n",
    "            if self.__check_transfer_learning(ratio_epochs, tl_coeff):\n",
    "                # Unfreeze all layers for fine-tuning\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                # Freeze the CNN part\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = False\n",
    "                # Unfreeze the classification layer\n",
    "                for param in self.model.get_fc().parameters():\n",
    "                    param.requires_grad = True\n",
    "        elif phase == \"val\":\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def __check_transfer_learning(self, ratio_epochs, tl_coeff=0):\n",
    "        return ratio_epochs >= tl_coeff\n",
    "\n",
    "    def __eval_model(self, dataloader, verbose=0):\n",
    "        model = self.model\n",
    "        criterion = self.criterion\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        if verbose > 0:\n",
    "            dataloader = tqdm(dataloader, desc=\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(self.DEVICE)\n",
    "                labels = labels.to(self.DEVICE)\n",
    "\n",
    "                if verbose > 2:\n",
    "                    print(f\"labels: {labels}\")\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(inputs)\n",
    "\n",
    "                if verbose > 3:\n",
    "                    print(f\"logit: {logits}\")\n",
    "\n",
    "                probs = F.softmax(logits, dim=1)  # Apply softmax to get probabilities\n",
    "\n",
    "                if verbose > 3:\n",
    "                    print(f\"probs: {probs}\")\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "                # Compute accuracy\n",
    "                predicted = torch.argmax(probs, dim=1)  # Get the class with the highest probability\n",
    "\n",
    "                if verbose > 2:\n",
    "                    print(f\"predicted: {predicted}\")\n",
    "                    print(f\"true/false: {(predicted == labels)}\")\n",
    "\n",
    "                correct_preds += (predicted == labels).sum().item()\n",
    "                total_preds += labels.size(0)\n",
    "\n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = val_loss / len(dataloader)\n",
    "        accuracy = correct_preds / total_preds\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def __save_model(self, training_epoch, verbose=0):\n",
    "        torch.save(self.model.state_dict(), self.save_folder + f\"/{training_epoch}b1_model.pth\")\n",
    "        if verbose > 0:\n",
    "            print(f\"Saved model to {self.save_folder}/b1_model.pth\")\n",
    "\n",
    "    def __save_checkpoint(self, epoch, model_state_dict, optimizer_state_dict, scheduler_state_dict=None, verbose=0):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,\n",
    "            'scheduler_state_dict': scheduler_state_dict\n",
    "        }\n",
    "        torch.save(checkpoint, self.save_folder + f'/checkpoint-epoch{epoch}.pth')\n",
    "        if verbose > 0:\n",
    "            print(f'Saved checkpoint to {self.save_folder}/checkpoint-epoch{epoch}.pth')\n",
    "\n",
    "    def __load_checkpoint(self, model, optimizer, checkpoint_path, verbose=0):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "\n",
    "        epoch = checkpoint['epoch']\n",
    "        model_state_dict = checkpoint['model_state_dict']\n",
    "        optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "        scheduler_state_dict = checkpoint['scheduler_state_dict']\n",
    "        model = model.load_state_dict(model_state_dict)\n",
    "        if self.scheduled:\n",
    "            optimizer.load_state_dict(optimizer_state_dict, scheduler_state_dict)\n",
    "        else:\n",
    "            optimizer = optimizer.load_state_dict(optimizer_state_dict)\n",
    "        return epoch, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702bc6e",
   "metadata": {
    "papermill": {
     "duration": 0.004032,
     "end_time": "2025-02-06T22:48:36.032506",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.028474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57558688",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.041549Z",
     "iopub.status.busy": "2025-02-06T22:48:36.041339Z",
     "iopub.status.idle": "2025-02-06T22:48:36.047728Z",
     "shell.execute_reply": "2025-02-06T22:48:36.047147Z"
    },
    "papermill": {
     "duration": 0.012265,
     "end_time": "2025-02-06T22:48:36.048928",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.036663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class B1Dataset(Dataset):\n",
    "\n",
    "    VIDEO_SPLITS = {\n",
    "        'train': {1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54},\n",
    "        'val': {0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51},\n",
    "        'test': {4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47}\n",
    "    }\n",
    "\n",
    "    def __init__(self, csv_file, split='train', transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.CenterCrop((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if split in self.VIDEO_SPLITS:\n",
    "            self.data = self.data[self.data['video_names'].astype(int).isin(self.VIDEO_SPLITS[split])]\n",
    "        else:\n",
    "            raise NameError(f'There is no such split: {split}, only {self.VIDEO_SPLITS}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['img_path']\n",
    "        label = self.data.iloc[idx]['Mapped_Label']\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d04530",
   "metadata": {
    "papermill": {
     "duration": 0.004036,
     "end_time": "2025-02-06T22:48:36.057204",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.053168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d5c002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.066485Z",
     "iopub.status.busy": "2025-02-06T22:48:36.066272Z",
     "iopub.status.idle": "2025-02-06T22:48:36.071338Z",
     "shell.execute_reply": "2025-02-06T22:48:36.070547Z"
    },
    "papermill": {
     "duration": 0.011308,
     "end_time": "2025-02-06T22:48:36.072613",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.061305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, dataset, device):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.device = device\n",
    "        weight = self.__compute_weights()\n",
    "        self.loss = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    def __compute_weights(self):\n",
    "        print('Computing weights...')\n",
    "        label_counts = Counter([label.item() for _, label in self.dataset])\n",
    "        total_samples = len(self.dataset)\n",
    "\n",
    "        class_weights = [total_samples / label_counts[i] if i in label_counts else 0 for i in range(8)]\n",
    "\n",
    "        weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n",
    "        print('Weights computed.')\n",
    "        return weights\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        return self.loss(logit, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e763f21",
   "metadata": {
    "papermill": {
     "duration": 0.004046,
     "end_time": "2025-02-06T22:48:36.080908",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.076862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aab286f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.089911Z",
     "iopub.status.busy": "2025-02-06T22:48:36.089682Z",
     "iopub.status.idle": "2025-02-06T22:48:36.094342Z",
     "shell.execute_reply": "2025-02-06T22:48:36.093747Z"
    },
    "papermill": {
     "duration": 0.010469,
     "end_time": "2025-02-06T22:48:36.095546",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.085077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class AdamWScheduled():\n",
    "    def __init__(self, model_params, lr, step_size, gamma):\n",
    "        self.optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model_params),  lr=lr)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size, gamma)\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def scheduler_step(self):\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def optimizer_state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def scheduler_state_dict(self):\n",
    "        return self.scheduler.state_dict()\n",
    "    \n",
    "    def load_state_dict(self, optimizer_state_dict, scheduler_state_dict):\n",
    "        self.optimizer.load_state_dict(optimizer_state_dict)\n",
    "        self.scheduler.load_state_dict(scheduler_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df01b01",
   "metadata": {
    "papermill": {
     "duration": 0.004255,
     "end_time": "2025-02-06T22:48:36.104445",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.100190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58fe0a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.113456Z",
     "iopub.status.busy": "2025-02-06T22:48:36.113261Z",
     "iopub.status.idle": "2025-02-06T22:48:36.165506Z",
     "shell.execute_reply": "2025-02-06T22:48:36.164702Z"
    },
    "papermill": {
     "duration": 0.058246,
     "end_time": "2025-02-06T22:48:36.166896",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.108650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36633ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.176824Z",
     "iopub.status.busy": "2025-02-06T22:48:36.176569Z",
     "iopub.status.idle": "2025-02-06T22:48:36.207249Z",
     "shell.execute_reply": "2025-02-06T22:48:36.206421Z"
    },
    "papermill": {
     "duration": 0.03713,
     "end_time": "2025-02-06T22:48:36.208618",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.171488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),            # Resize shorter side to 256     \n",
    "    transforms.RandomResizedCrop(224),  # Randomly crop and resize to 224x224\n",
    "    transforms.RandomRotation(degrees=5),                   # Randomly rotate images within ±5 degrees\n",
    "    transforms.ToTensor(),                                   # Convert PIL images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],        # Normalize using ImageNet mean and std values\n",
    "                         std=[0.229, 0.224, 0.225]),         # (mean and std are the same used during ResNet pre-training)\n",
    "])\n",
    "\n",
    "train_dataset = B1Dataset(csv_file='/kaggle/working/dataset.csv/dataset.csv', split='train', transform=train_transform)\n",
    "val_dataset = B1Dataset(csv_file='/kaggle/working/dataset.csv/dataset.csv', split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1ae6e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.218000Z",
     "iopub.status.busy": "2025-02-06T22:48:36.217758Z",
     "iopub.status.idle": "2025-02-06T22:48:36.221805Z",
     "shell.execute_reply": "2025-02-06T22:48:36.221030Z"
    },
    "papermill": {
     "duration": 0.010156,
     "end_time": "2025-02-06T22:48:36.223140",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.212984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8081e112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.232364Z",
     "iopub.status.busy": "2025-02-06T22:48:36.232166Z",
     "iopub.status.idle": "2025-02-06T22:48:36.357718Z",
     "shell.execute_reply": "2025-02-06T22:48:36.356608Z"
    },
    "papermill": {
     "duration": 0.131764,
     "end_time": "2025-02-06T22:48:36.359136",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.227372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir '/kaggle/working/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b412b1f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:48:36.368896Z",
     "iopub.status.busy": "2025-02-06T22:48:36.368651Z",
     "iopub.status.idle": "2025-02-06T22:49:53.460504Z",
     "shell.execute_reply": "2025-02-06T22:49:53.459601Z"
    },
    "papermill": {
     "duration": 77.102695,
     "end_time": "2025-02-06T22:49:53.466438",
     "exception": false,
     "start_time": "2025-02-06T22:48:36.363743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing weights...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights computed.\n"
     ]
    }
   ],
   "source": [
    "criterion = WeightedCrossEntropyLoss(train_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d6019f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:49:53.477195Z",
     "iopub.status.busy": "2025-02-06T22:49:53.476956Z",
     "iopub.status.idle": "2025-02-06T22:49:54.636047Z",
     "shell.execute_reply": "2025-02-06T22:49:54.635081Z"
    },
    "papermill": {
     "duration": 1.165673,
     "end_time": "2025-02-06T22:49:54.637630",
     "exception": false,
     "start_time": "2025-02-06T22:49:53.471957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 16.8M/97.8M [00:00<00:00, 175MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▋      | 35.6M/97.8M [00:00<00:00, 188MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 54.2M/97.8M [00:00<00:00, 191MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▍  | 72.9M/97.8M [00:00<00:00, 192MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 91.4M/97.8M [00:00<00:00, 193MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 192MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ResnetEvolution(hidden_layers=[])\n",
    "model = model.to(device)\n",
    "optimizer = AdamWScheduled(model_params=model.parameters(), lr=0.001, step_size=2, gamma=0.6)\n",
    "\n",
    "save_folder = '/kaggle/working/'\n",
    "trainer = b1_ModelTrainer(model, optimizer,True, criterion, epochs=50, dataloaders=dataloaders, device=device, save_folder=save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11a8efc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:49:54.648907Z",
     "iopub.status.busy": "2025-02-06T22:49:54.648653Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-02-06T22:49:54.643147",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, train Loss: 1.8892422437667846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, (val) Loss: 3.126393530103895 | Accuracy: 0.20879940343027592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, train Loss: 1.4687151908874512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, (val) Loss: 4.405807548099094 | Accuracy: 0.31767337807606266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, train Loss: 1.1811416586240133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, (val) Loss: 1.2807031207614474 | Accuracy: 0.5242356450410142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, train Loss: 0.9996740221977234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, (val) Loss: 1.3622241814931233 | Accuracy: 0.506338553318419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, train Loss: 0.8742741187413533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, (val) Loss: 0.9742305874824524 | Accuracy: 0.6167039522744221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, train Loss: 0.8482908725738525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, (val) Loss: 1.7791730297936335 | Accuracy: 0.42878448918717377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, train Loss: 0.7699017643928527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, (val) Loss: 0.8259660734070672 | Accuracy: 0.6718866517524236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, train Loss: 0.7412206013997396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, (val) Loss: 0.8682983385192024 | Accuracy: 0.6733780760626398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, train Loss: 0.7040615479151408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, (val) Loss: 0.8350306087070041 | Accuracy: 0.6890380313199105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, train Loss: 0.6740531961123148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, (val) Loss: 0.7811700569258796 | Accuracy: 0.6927665920954511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, train Loss: 0.6485157370567322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, (val) Loss: 0.800147083070543 | Accuracy: 0.70917225950783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, train Loss: 0.6476048072179158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, (val) Loss: 0.777892013390859 | Accuracy: 0.7076808351976137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, train Loss: 0.6097591141859691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, (val) Loss: 0.7564597196049161 | Accuracy: 0.7158836689038032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, train Loss: 0.6057929833730061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, (val) Loss: 0.7634733650419447 | Accuracy: 0.7166293810589113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, train Loss: 0.6015687227249146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, (val) Loss: 0.7702733609411452 | Accuracy: 0.7181208053691275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, train Loss: 0.5822302957375844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, (val) Loss: 0.7802297936545478 | Accuracy: 0.7166293810589113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, train Loss: 0.5998337169488271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, (val) Loss: 0.7697478996382819 | Accuracy: 0.7218493661446681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, train Loss: 0.5717344363530477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, (val) Loss: 0.786088764667511 | Accuracy: 0.7203579418344519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, train Loss: 0.5523363212744395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, (val) Loss: 0.7846739027235243 | Accuracy: 0.72110365398956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, train Loss: 0.5581877966721852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, (val) Loss: 0.7942999667591519 | Accuracy: 0.7218493661446681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, train Loss: 0.5681642949581146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, (val) Loss: 0.7961466047498915 | Accuracy: 0.7248322147651006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, train Loss: 0.5364772359530131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, (val) Loss: 0.7882953815990024 | Accuracy: 0.7218493661446681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, train Loss: 0.5495124876499176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, (val) Loss: 0.7861086924870809 | Accuracy: 0.7263236390753169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, train Loss: 0.5809981266657511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, (val) Loss: 0.7758278581831191 | Accuracy: 0.7248322147651006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, train Loss: 0.5579786777496338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, (val) Loss: 0.7754461897744073 | Accuracy: 0.7285607755406414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, train Loss: 0.543040390809377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, (val) Loss: 0.7765973740153842 | Accuracy: 0.7263236390753169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, train Loss: 0.5377838412920634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, (val) Loss: 0.7814965181880527 | Accuracy: 0.7255779269202088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, train Loss: 0.5837013820807139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, (val) Loss: 0.7850763069258796 | Accuracy: 0.7248322147651006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, train Loss: 0.5460118889808655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, (val) Loss: 0.7696648902363248 | Accuracy: 0.7293064876957495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, train Loss: 0.5493525405724843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, (val) Loss: 0.7805676195356581 | Accuracy: 0.7225950782997763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, train Loss: 0.528934492667516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, (val) Loss: 0.7836166752709283 | Accuracy: 0.7263236390753169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, train Loss: 0.5281510730584462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, (val) Loss: 0.7799205515119765 | Accuracy: 0.727069351230425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, train Loss: 0.5526062508424123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, (val) Loss: 0.7770122223430209 | Accuracy: 0.7278150633855331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, train Loss: 0.5599077542622884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, (val) Loss: 0.7705989546245999 | Accuracy: 0.7240865026099925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, train Loss: 0.5522291521231334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, (val) Loss: 0.7810366219944425 | Accuracy: 0.7225950782997763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training epoch 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, train Loss: 0.5456225593884786\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_accuracies = trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8a701",
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-06T20:51:45.455Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotter.plot_training_val_b1(train_losses, val_losses, val_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5561212,
     "sourceId": 9198495,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-06T22:48:24.523195",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}