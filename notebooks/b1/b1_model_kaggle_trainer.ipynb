{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9198495,"sourceType":"datasetVersion","datasetId":5561212}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"79f8ef07aec13169","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom PIL import Image\nimport torchvision\nimport os\nfrom torchvision.transforms import v2\nimport glob\nimport torch.nn.functional as F\nfrom torchvision.models import ResNet50_Weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:40:26.877481Z","iopub.execute_input":"2025-02-06T05:40:26.877776Z","iopub.status.idle":"2025-02-06T05:40:26.882798Z","shell.execute_reply.started":"2025-02-06T05:40:26.877753Z","shell.execute_reply":"2025-02-06T05:40:26.881922Z"}},"outputs":[],"execution_count":39},{"id":"8918a8051eea0c8b","cell_type":"markdown","source":"## Data Processing","metadata":{}},{"id":"4ac80a6719fe945","cell_type":"code","source":"class AnnotationProcessor:\n    def __init__(self, base_path, output_path='/kaggle/working/', filename='dataset.csv'):\n        self.base_path = base_path\n        self.output_path = output_path\n        self.filename = filename\n        self.data = None\n        self.run()\n\n    def get_group_annotation(self, file, folder_name):\n        \"\"\"Extract the first two elements from each row of the annotation file.\"\"\"\n        with open(file, 'r') as f:\n            data = [line.split()[:2] for line in f]\n\n        df = pd.DataFrame(data, columns=['FrameID', 'Label'])\n        df['video_names'] = folder_name\n## ['l-spike', 'l_set', 'r_set', 'r-pass', 'r_spike', 'l-pass',\n       #'r_winpoint', 'l_winpoint']\n        label_mapping = {'l-spike': 0, 'l_set': 1, 'r_set': 2, 'r-pass': 3, 'r_spike': 4, 'l-pass': 5, 'r_winpoint': 6, 'l_winpoint': 7}\n        df['Mapped_Label'] = df['Label'].map(label_mapping).astype('int64')\n        # Ensure the output directory exists\n        os.makedirs(self.output_path, exist_ok=True)\n        # Save the file directly in the root of the output path\n        df.to_csv(os.path.join(self.output_path, f'{folder_name}.csv'), index=False)\n\n    def process_annotations(self):\n        \"\"\"Process annotations from all folders in the base path.\"\"\"\n        for folder_name in os.listdir(self.base_path):\n            folder_path = os.path.join(self.base_path, folder_name)\n            if os.path.isdir(folder_path):\n                annotated_file_path = os.path.join(folder_path, 'annotations.txt')\n                self.get_group_annotation(annotated_file_path, folder_name)\n\n    def combine_csv_files(self):\n        \"\"\"Combine all CSV files into a single DataFrame.\"\"\"\n        csv_files = glob.glob(os.path.join(self.output_path, '*.csv'))\n        data = [pd.read_csv(csv_file) for csv_file in csv_files]\n        for d in data:\n            d.dropna(inplace=True)\n            # print(d.isna().sum())\n        return pd.concat(data, ignore_index=True)\n\n    def generate_img_paths(self, df):\n        \"\"\"Generate image paths based on the DataFrame.\"\"\"\n        df['img_path'] = df.apply(\n            lambda x: os.path.join(\n                self.base_path,\n                str(x['video_names']),  # Ensure `video_names` is a string\n                str(x['FrameID'])[:-4],  # Ensure `FrameID` is a string and remove the last 4 characters\n                str(x['FrameID'])  # Ensure `FrameID` is a string\n            ), axis=1\n        )\n        return df\n\n    def save_combined_data(self, df):\n        \"\"\"Save the combined data to a CSV file.\"\"\"\n        # Ensure the output directory exists before saving\n        os.makedirs(self.output_path, exist_ok=True)\n        # Save the combined data directly in the root of the output path\n        df.to_csv(os.path.join(self.output_path, self.filename), index=False)\n\n    def cleanup(self):\n        \"\"\"Delete all intermediate CSV files except the final output file.\"\"\"\n        for csv_file in glob.glob(os.path.join(self.output_path, '*.csv')):\n            if not csv_file.endswith(self.filename):\n                os.remove(csv_file)\n\n    def run(self):\n        \"\"\"Run the whole annotation processing pipeline.\"\"\"\n        self.process_annotations()\n        combined_data = self.combine_csv_files()\n        data_with_paths = self.generate_img_paths(combined_data)\n        self.save_combined_data(data_with_paths)\n        self.cleanup()  # Clean up intermediate files\n        self.data = pd.read_csv(os.path.join(self.output_path, self.filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:08:03.835136Z","iopub.execute_input":"2025-02-06T05:08:03.835647Z","iopub.status.idle":"2025-02-06T05:08:03.846883Z","shell.execute_reply.started":"2025-02-06T05:08:03.835613Z","shell.execute_reply":"2025-02-06T05:08:03.845809Z"}},"outputs":[],"execution_count":2},{"id":"5e415378-0d48-4630-a898-59816140439a","cell_type":"markdown","source":"Make sure you want to run this before running it","metadata":{}},{"id":"48b50a09-8e04-4cee-86f3-60cc8f4e7b9c","cell_type":"code","source":"!rm -rf '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:07:09.087775Z","iopub.execute_input":"2025-02-06T05:07:09.087963Z","iopub.status.idle":"2025-02-06T05:07:09.234051Z","shell.execute_reply.started":"2025-02-06T05:07:09.087947Z","shell.execute_reply":"2025-02-06T05:07:09.233249Z"}},"outputs":[{"name":"stdout","text":"shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\nrm: cannot remove '/kaggle/working/': Device or resource busy\n","output_type":"stream"}],"execution_count":30},{"id":"e20e0216b604d15f","cell_type":"code","source":"base_path = '/kaggle/input/volleyball/volleyball_/videos/'\ndf = AnnotationProcessor(base_path, 'dataset.csv').data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:08:11.235679Z","iopub.execute_input":"2025-02-06T05:08:11.236071Z","iopub.status.idle":"2025-02-06T05:08:12.030906Z","shell.execute_reply.started":"2025-02-06T05:08:11.236036Z","shell.execute_reply":"2025-02-06T05:08:12.030214Z"}},"outputs":[],"execution_count":3},{"id":"ba66d653c7772881","cell_type":"markdown","source":"## Model","metadata":{}},{"id":"initial_id","cell_type":"code","source":"class ResnetEvolution(nn.Module):\n    def __init__(self, hidden_layers=[128, 64, 32]):\n        super(ResnetEvolution, self).__init__()\n        self.hidden_layers = hidden_layers\n        self.model = self.__init_backbone(torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2))\n\n    def __init_backbone(self, backbone):\n        num_features = backbone.fc.in_features\n\n        layers = []\n        input_size = num_features  # Start with backbone output size\n        for hidden_size in self.hidden_layers:\n            layers.append(nn.Linear(input_size, hidden_size))\n            layers.append(nn.ReLU())  # Activation function\n            input_size = hidden_size  # Update input for next layer\n\n        layers.append(nn.Linear(input_size, 8))  # Final output layer\n\n        backbone.fc = nn.Sequential(*layers)  # Output layer for binary classification\n\n        return backbone\n\n    def get_fc(self):\n        return self.model.fc\n\n    def forward(self, images):\n        return self.model(images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:40:53.050369Z","iopub.execute_input":"2025-02-06T05:40:53.050684Z","iopub.status.idle":"2025-02-06T05:40:53.056831Z","shell.execute_reply.started":"2025-02-06T05:40:53.050653Z","shell.execute_reply":"2025-02-06T05:40:53.055798Z"}},"outputs":[],"execution_count":41},{"id":"f7d5105b43c23b82","cell_type":"markdown","source":"## Trainer","metadata":{}},{"id":"e8b540404fed68a","cell_type":"code","source":"class b1_ModelTrainer:\n    def __init__(self, model, optimizer, criterion, epochs, dataloaders, device, save_folder, is_continue=False, checkpoint=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.epochs = epochs\n        self.dataloaders = dataloaders\n        self.DEVICE = device\n        self.save_folder = save_folder\n        self.is_continue = is_continue\n        self.checkpoint = checkpoint\n\n    def train_model(self):\n        model, optimizer, criterion, epochs, dataloaders = self.model, self.optimizer, self.criterion, self.epochs, self.dataloaders\n\n        epoch = 0\n        if self.is_continue:\n            epoch, model, optimizer = self.__load_checkpoint(model, optimizer, self.checkpoint)\n\n        for training_epoch in range(epoch, epochs):\n            print(f\"\\nTraining epoch {training_epoch}: training {'full model' if self.__check_transfer_learning(training_epoch/epochs) else 'only fc model'}\")\n            ## change model mode depending on the phase\n            for phase in ['train', 'val']:\n                dataloader = dataloaders[phase]\n                epoch_loss = 0  # Track total loss for the epoch\n                if phase == 'train':\n                    model.train()\n                    for inputs, labels in tqdm(dataloader, desc=phase):\n                        inputs = inputs.to(self.DEVICE)\n                        labels = labels.to(self.DEVICE)\n                        # zero grads of he optim\n                        optimizer.zero_grad()\n                        # freeze the non-learnable weights\n                        self.__handle_transfer_learning(phase, training_epoch / epochs)\n                        # forward pass\n                        logit = model(inputs)\n                        loss = criterion(logit, labels)\n                        loss.backward()\n                        # update weights\n                        optimizer.step()\n                        epoch_loss += loss.item()  # Accumulate loss\n                    print(\n                        f\"Epoch {training_epoch + 1}/{epochs}, {phase} Loss: {epoch_loss / len(dataloader)}\")  # Print loss\n                else:\n                    # skip evaluation if no suitable dataloader\n                    if dataloaders[phase] is None:\n                        continue\n                    model.eval()\n                    loss, acc = self.__eval_model(dataloader)\n                    print(f\"Epoch {training_epoch + 1}/{epochs}, ({phase}) Loss: {loss} | Accuracy: {acc}\")  # Print loss\n\n\n            self.__save_checkpoint(training_epoch, model.state_dict(), optimizer.state_dict())\n\n        self.__save_model()\n\n    def __handle_transfer_learning(self, phase, ratio_epochs, tl_coeff=0.8):\n        if phase == \"train\":\n            if self.__check_transfer_learning(ratio_epochs, tl_coeff):\n                # Unfreeze all layers for fine-tuning\n                for param in self.model.parameters():\n                    param.requires_grad = True\n            else:\n                # Freeze the CNN part\n                for param in self.model.parameters():\n                    param.requires_grad = False\n                # Unfreeze the classification layer\n                for param in self.model.get_fc().parameters():\n                    param.requires_grad = True\n        elif phase == \"val\":\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n    def __check_transfer_learning(self, ratio_epochs, tl_coeff=0.8):\n        return ratio_epochs >= tl_coeff\n\n    def __eval_model(self, dataloader):\n        model = self.model\n        criterion = self.criterion\n        model.eval()\n        val_loss = 0\n        correct_preds = 0\n        total_preds = 0\n\n        with torch.no_grad():\n            for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n                inputs = inputs.to(self.DEVICE)\n                labels = labels.to(self.DEVICE)\n\n                # Forward pass\n                logits = model(inputs)\n                probs = F.softmax(logits, dim=1)  # Apply softmax to get probabilities\n                loss = criterion(logits, labels)\n                val_loss += loss.item()  # Accumulate loss\n\n                # Compute accuracy\n                predicted = torch.argmax(probs, dim=1)  # Get the class with the highest probability\n                correct_preds += (predicted == labels).sum().item()\n                total_preds += labels.size(0)\n\n        # Calculate average loss and accuracy\n        avg_loss = val_loss / len(dataloader)\n        accuracy = correct_preds / total_preds\n        return avg_loss, accuracy\n\n    def __save_model(self):\n        torch.save(self.model.state_dict(), self.save_folder + \"/b1_model.pth\")\n\n    def __save_checkpoint(self, epoch, model_state_dict, optimizer_state_dict):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model_state_dict,\n            'optimizer_state_dict': optimizer_state_dict,\n        }\n        torch.save(checkpoint, self.save_folder + f'/checkpoint-epoch{epoch}.pth')\n\n    def __load_checkpoint(self, model, optimizer, checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        epoch = checkpoint['epoch']\n        model_state_dict = checkpoint['model_state_dict']\n        optimizer_state_dict = checkpoint['optimizer_state_dict']\n        model = model.load_state_dict(model_state_dict)\n        optimizer = optimizer.load_state_dict(optimizer_state_dict)\n        return epoch, model, optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:33:44.411074Z","iopub.execute_input":"2025-02-06T05:33:44.411383Z","iopub.status.idle":"2025-02-06T05:33:44.424938Z","shell.execute_reply.started":"2025-02-06T05:33:44.411356Z","shell.execute_reply":"2025-02-06T05:33:44.423936Z"}},"outputs":[],"execution_count":34},{"id":"d2e0e7432bcf15ee","cell_type":"markdown","source":"## Dataset","metadata":{}},{"id":"5fc01b6abf2a455b","cell_type":"code","source":"class B1Dataset(Dataset):\n\n    VIDEO_SPLITS = {\n        'train': {1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54},\n        'val': {0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51},\n        'test': {4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47}\n    }\n\n    def __init__(self, csv_file, split='train', transform=None):\n        self.data = pd.read_csv(csv_file)\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((256, 256)),\n                transforms.CenterCrop((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n        else:\n            self.transform = transform\n\n        if split in self.VIDEO_SPLITS:\n            self.data = self.data[self.data['video_names'].astype(int).isin(self.VIDEO_SPLITS[split])]\n        else:\n            raise NameError(f'There is no such split: {split}, only {self.VIDEO_SPLITS}')\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path = self.data.iloc[idx]['img_path']\n        label = self.data.iloc[idx]['Mapped_Label']\n\n        # Load image\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Apply transformations if provided\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:08:28.494635Z","iopub.execute_input":"2025-02-06T05:08:28.494916Z","iopub.status.idle":"2025-02-06T05:08:28.503178Z","shell.execute_reply.started":"2025-02-06T05:08:28.494896Z","shell.execute_reply":"2025-02-06T05:08:28.502216Z"}},"outputs":[],"execution_count":6},{"id":"136b66cec80ecaf6","cell_type":"markdown","source":"## Code","metadata":{}},{"id":"8b5e4d106b988ef4","cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:08:31.150100Z","iopub.execute_input":"2025-02-06T05:08:31.150404Z","iopub.status.idle":"2025-02-06T05:08:31.209902Z","shell.execute_reply.started":"2025-02-06T05:08:31.150369Z","shell.execute_reply":"2025-02-06T05:08:31.208937Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":7},{"id":"2ea1f05e86b1d61c","cell_type":"code","source":"train_dataset = B1Dataset(csv_file='/kaggle/working/dataset.csv/dataset.csv', split='train')\nval_dataset = B1Dataset(csv_file='/kaggle/working/dataset.csv/dataset.csv', split='val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:08:32.952842Z","iopub.execute_input":"2025-02-06T05:08:32.953380Z","iopub.status.idle":"2025-02-06T05:08:32.988151Z","shell.execute_reply.started":"2025-02-06T05:08:32.953331Z","shell.execute_reply":"2025-02-06T05:08:32.987290Z"}},"outputs":[],"execution_count":8},{"id":"7b115b6111f9f235","cell_type":"code","source":"batch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:08:35.013694Z","iopub.execute_input":"2025-02-06T05:08:35.014171Z","iopub.status.idle":"2025-02-06T05:08:35.020402Z","shell.execute_reply.started":"2025-02-06T05:08:35.014126Z","shell.execute_reply":"2025-02-06T05:08:35.019327Z"}},"outputs":[],"execution_count":9},{"id":"84b5edf0-20db-4d09-ad38-989254833873","cell_type":"code","source":"!mkdir '/kaggle/working/checkpoints/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:07:09.260849Z","iopub.status.idle":"2025-02-06T05:07:09.261240Z","shell.execute_reply":"2025-02-06T05:07:09.261121Z"}},"outputs":[],"execution_count":null},{"id":"b8d7493cca425a71","cell_type":"code","source":"model = ResnetEvolution(hidden_layers=[4096, 2048, 1024, 512, 256, 128, 64])\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nsave_folder = '/kaggle/working/'\ntrainer = b1_ModelTrainer(model, optimizer, criterion, epochs=5, dataloaders=dataloaders, device=device, save_folder=save_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:40:57.632442Z","iopub.execute_input":"2025-02-06T05:40:57.632743Z","iopub.status.idle":"2025-02-06T05:40:58.876933Z","shell.execute_reply.started":"2025-02-06T05:40:57.632719Z","shell.execute_reply":"2025-02-06T05:40:58.876037Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 189MB/s] \n","output_type":"stream"}],"execution_count":42},{"id":"b8cf55c0-1d33-43f3-9f74-6f5cd56076bb","cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:41:02.528480Z","iopub.execute_input":"2025-02-06T05:41:02.528803Z","iopub.status.idle":"2025-02-06T05:41:02.535132Z","shell.execute_reply.started":"2025-02-06T05:41:02.528774Z","shell.execute_reply":"2025-02-06T05:41:02.534408Z"}},"outputs":[{"name":"stdout","text":"ResnetEvolution(\n  (model): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Sequential(\n      (0): Linear(in_features=2048, out_features=4096, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=4096, out_features=2048, bias=True)\n      (3): ReLU()\n      (4): Linear(in_features=2048, out_features=1024, bias=True)\n      (5): ReLU()\n      (6): Linear(in_features=1024, out_features=512, bias=True)\n      (7): ReLU()\n      (8): Linear(in_features=512, out_features=256, bias=True)\n      (9): ReLU()\n      (10): Linear(in_features=256, out_features=128, bias=True)\n      (11): ReLU()\n      (12): Linear(in_features=128, out_features=64, bias=True)\n      (13): ReLU()\n      (14): Linear(in_features=64, out_features=8, bias=True)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":43},{"id":"1547b20186ca6e47","cell_type":"code","source":"trainer.train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T05:41:08.108505Z","iopub.execute_input":"2025-02-06T05:41:08.108931Z","iopub.status.idle":"2025-02-06T05:46:21.256581Z","shell.execute_reply.started":"2025-02-06T05:41:08.108895Z","shell.execute_reply":"2025-02-06T05:46:21.255508Z"}},"outputs":[{"name":"stdout","text":"\nTraining epoch 0: training only fc model\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:38<00:00,  1.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, train Loss: 2.0532064402804657\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 42/42 [00:22<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, (val) Loss: 2.042787883962904 | Accuracy: 0.16554809843400448\n\nTraining epoch 1: training only fc model\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:38<00:00,  1.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, train Loss: 2.002999293453553\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 42/42 [00:21<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, (val) Loss: 2.0227622304643904 | Accuracy: 0.174496644295302\n\nTraining epoch 2: training only fc model\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:37<00:00,  1.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, train Loss: 1.9432735092499678\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 42/42 [00:20<00:00,  2.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, (val) Loss: 2.053564088685172 | Accuracy: 0.14988814317673377\n\nTraining epoch 3: training only fc model\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:37<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, train Loss: 1.900316818672068\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 42/42 [00:22<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, (val) Loss: 2.0022282742318653 | Accuracy: 0.19164802386278895\n\nTraining epoch 4: training full model\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:46<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, train Loss: 1.854139673359254\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 42/42 [00:22<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, (val) Loss: 2.1053259060496377 | Accuracy: 0.21551081282624907\n","output_type":"stream"}],"execution_count":44}]}