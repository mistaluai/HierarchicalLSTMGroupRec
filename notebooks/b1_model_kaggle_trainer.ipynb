{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9198495,"sourceType":"datasetVersion","datasetId":5561212}],"dockerImageVersionId":30841,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"79f8ef07aec13169","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom PIL import Image\nimport torchvision\nimport os\nfrom torchvision.transforms import v2\nimport glob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T21:31:28.383709Z","iopub.execute_input":"2025-02-05T21:31:28.384100Z","iopub.status.idle":"2025-02-05T21:31:28.388947Z","shell.execute_reply.started":"2025-02-05T21:31:28.384073Z","shell.execute_reply":"2025-02-05T21:31:28.388021Z"}},"outputs":[],"execution_count":11},{"id":"8918a8051eea0c8b","cell_type":"markdown","source":"## Data Processing","metadata":{}},{"id":"4ac80a6719fe945","cell_type":"code","source":"class AnnotationProcessor:\n    def __init__(self, base_path, output_path='/kaggle/working/', filename='dataset.csv'):\n        self.base_path = base_path\n        self.output_path = output_path\n        self.filename = filename\n        self.data = None\n        self.run()\n\n    def get_group_annotation(self, file, folder_name):\n        \"\"\"Extract the first two elements from each row of the annotation file.\"\"\"\n        with open(file, 'r') as f:\n            data = [line.split()[:2] for line in f]\n\n        df = pd.DataFrame(data, columns=['FrameID', 'Label'])\n        df['video_names'] = folder_name\n## ['l-spike', 'l_set', 'r_set', 'r-pass', 'r_spike', 'l-pass',\n       #'r_winpoint', 'l_winpoint']\n        label_mapping = {'l-spike': 0, 'l_set': 1, 'r_set': 2, 'r-pass': 3, 'r_spike': 4, 'l-pass': 5, 'r_winpoint': 6, 'l_winpoint': 7}\n        df['Mapped_Label'] = df['Label'].map(label_mapping).astype('int64')\n        # Ensure the output directory exists\n        os.makedirs(self.output_path, exist_ok=True)\n        # Save the file directly in the root of the output path\n        df.to_csv(os.path.join(self.output_path, f'{folder_name}.csv'), index=False)\n\n    def process_annotations(self):\n        \"\"\"Process annotations from all folders in the base path.\"\"\"\n        for folder_name in os.listdir(self.base_path):\n            folder_path = os.path.join(self.base_path, folder_name)\n            if os.path.isdir(folder_path):\n                annotated_file_path = os.path.join(folder_path, 'annotations.txt')\n                self.get_group_annotation(annotated_file_path, folder_name)\n\n    def combine_csv_files(self):\n        \"\"\"Combine all CSV files into a single DataFrame.\"\"\"\n        csv_files = glob.glob(os.path.join(self.output_path, '*.csv'))\n        data = [pd.read_csv(csv_file) for csv_file in csv_files]\n        for d in data:\n            d.dropna(inplace=True)\n            # print(d.isna().sum())\n        return pd.concat(data, ignore_index=True)\n\n    def generate_img_paths(self, df):\n        \"\"\"Generate image paths based on the DataFrame.\"\"\"\n        df['img_path'] = df.apply(\n            lambda x: os.path.join(\n                self.base_path,\n                str(x['video_names']),  # Ensure `video_names` is a string\n                str(x['FrameID'])[:-4],  # Ensure `FrameID` is a string and remove the last 4 characters\n                str(x['FrameID'])  # Ensure `FrameID` is a string\n            ), axis=1\n        )\n        return df\n\n    def save_combined_data(self, df):\n        \"\"\"Save the combined data to a CSV file.\"\"\"\n        # Ensure the output directory exists before saving\n        os.makedirs(self.output_path, exist_ok=True)\n        # Save the combined data directly in the root of the output path\n        df.to_csv(os.path.join(self.output_path, self.filename), index=False)\n\n    def cleanup(self):\n        \"\"\"Delete all intermediate CSV files except the final output file.\"\"\"\n        for csv_file in glob.glob(os.path.join(self.output_path, '*.csv')):\n            if not csv_file.endswith(self.filename):\n                os.remove(csv_file)\n\n    def run(self):\n        \"\"\"Run the whole annotation processing pipeline.\"\"\"\n        self.process_annotations()\n        combined_data = self.combine_csv_files()\n        data_with_paths = self.generate_img_paths(combined_data)\n        self.save_combined_data(data_with_paths)\n        self.cleanup()  # Clean up intermediate files\n        self.data = pd.read_csv(os.path.join(self.output_path, self.filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:10:16.705143Z","iopub.execute_input":"2025-02-05T22:10:16.705457Z","iopub.status.idle":"2025-02-05T22:10:16.715877Z","shell.execute_reply.started":"2025-02-05T22:10:16.705431Z","shell.execute_reply":"2025-02-05T22:10:16.714999Z"}},"outputs":[],"execution_count":94},{"id":"e20e0216b604d15f","cell_type":"code","source":"!rm -rf '/kaggle/working/dataset.csv'\nbase_path = '/kaggle/input/volleyball/volleyball_/videos/'\nAnnotationProcessor(base_path, 'dataset.csv').data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:10:38.429823Z","iopub.execute_input":"2025-02-05T22:10:38.430204Z","iopub.status.idle":"2025-02-05T22:10:38.924605Z","shell.execute_reply.started":"2025-02-05T22:10:38.430177Z","shell.execute_reply":"2025-02-05T22:10:38.923734Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"        FrameID       Label  video_names  Mapped_Label  \\\n0     11190.jpg     l-spike           52             0   \n1     10975.jpg     l-spike           52             0   \n2     11155.jpg       l_set           52             1   \n3      4350.jpg       l_set           52             1   \n4     15360.jpg       r_set           52             2   \n...         ...         ...          ...           ...   \n4825  35550.jpg  l_winpoint           49             7   \n4826  32745.jpg  r_winpoint           49             6   \n4827  36900.jpg  l_winpoint           49             7   \n4828  46905.jpg  r_winpoint           49             6   \n4829  42810.jpg  l_winpoint           49             7   \n\n                                               img_path  \n0     /kaggle/input/volleyball/volleyball_/videos/52...  \n1     /kaggle/input/volleyball/volleyball_/videos/52...  \n2     /kaggle/input/volleyball/volleyball_/videos/52...  \n3     /kaggle/input/volleyball/volleyball_/videos/52...  \n4     /kaggle/input/volleyball/volleyball_/videos/52...  \n...                                                 ...  \n4825  /kaggle/input/volleyball/volleyball_/videos/49...  \n4826  /kaggle/input/volleyball/volleyball_/videos/49...  \n4827  /kaggle/input/volleyball/volleyball_/videos/49...  \n4828  /kaggle/input/volleyball/volleyball_/videos/49...  \n4829  /kaggle/input/volleyball/volleyball_/videos/49...  \n\n[4830 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FrameID</th>\n      <th>Label</th>\n      <th>video_names</th>\n      <th>Mapped_Label</th>\n      <th>img_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11190.jpg</td>\n      <td>l-spike</td>\n      <td>52</td>\n      <td>0</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/52...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10975.jpg</td>\n      <td>l-spike</td>\n      <td>52</td>\n      <td>0</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/52...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11155.jpg</td>\n      <td>l_set</td>\n      <td>52</td>\n      <td>1</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/52...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4350.jpg</td>\n      <td>l_set</td>\n      <td>52</td>\n      <td>1</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/52...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15360.jpg</td>\n      <td>r_set</td>\n      <td>52</td>\n      <td>2</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/52...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4825</th>\n      <td>35550.jpg</td>\n      <td>l_winpoint</td>\n      <td>49</td>\n      <td>7</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/49...</td>\n    </tr>\n    <tr>\n      <th>4826</th>\n      <td>32745.jpg</td>\n      <td>r_winpoint</td>\n      <td>49</td>\n      <td>6</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/49...</td>\n    </tr>\n    <tr>\n      <th>4827</th>\n      <td>36900.jpg</td>\n      <td>l_winpoint</td>\n      <td>49</td>\n      <td>7</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/49...</td>\n    </tr>\n    <tr>\n      <th>4828</th>\n      <td>46905.jpg</td>\n      <td>r_winpoint</td>\n      <td>49</td>\n      <td>6</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/49...</td>\n    </tr>\n    <tr>\n      <th>4829</th>\n      <td>42810.jpg</td>\n      <td>l_winpoint</td>\n      <td>49</td>\n      <td>7</td>\n      <td>/kaggle/input/volleyball/volleyball_/videos/49...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4830 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":95},{"id":"ba66d653c7772881","cell_type":"markdown","source":"## Model","metadata":{}},{"id":"initial_id","cell_type":"code","source":"class ResnetEvolution(nn.Module):\n    def __init__(self, hidden_layers=[128, 64, 32]):\n        super(ResnetEvolution, self).__init__()\n        self.hidden_layers = hidden_layers\n        self.model = self.__init_backbone(torchvision.models.resnet50(pretrained=True))\n        self.fc = self.model.fc \n\n    def __init_backbone(self, backbone):\n        num_features = backbone.fc.in_features\n\n        layers = []\n        input_size = num_features  # Start with backbone output size\n        for hidden_size in self.hidden_layers:\n            layers.append(nn.Linear(input_size, hidden_size))\n            layers.append(nn.ReLU())  # Activation function\n            input_size = hidden_size  # Update input for next layer\n\n        layers.append(nn.Linear(input_size, 8))  # Final output layer\n\n        backbone.fc = nn.Sequential(*layers)  # Output layer for binary classification\n\n        return backbone\n\n    def forward(self, images):\n        return self.model(images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:10:57.826145Z","iopub.execute_input":"2025-02-05T22:10:57.826449Z","iopub.status.idle":"2025-02-05T22:10:57.832603Z","shell.execute_reply.started":"2025-02-05T22:10:57.826426Z","shell.execute_reply":"2025-02-05T22:10:57.831743Z"}},"outputs":[],"execution_count":96},{"id":"f7d5105b43c23b82","cell_type":"markdown","source":"## Trainer","metadata":{}},{"id":"e8b540404fed68a","cell_type":"code","source":"class b1_ModelTrainer:\n    def __init__(self, model, optimizer, criterion, epochs, dataloaders, device, save_folder, is_continue=False, checkpoint=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.epochs = epochs\n        self.dataloaders = dataloaders\n        self.DEVICE = device\n        self.save_folder = save_folder\n        self.is_continue = is_continue\n        self.checkpoint = checkpoint\n\n    def train_model(self):\n        model, optimizer, criterion, epochs, dataloaders = self.model, self.optimizer, self.criterion, self.epochs, self.dataloaders\n\n        epoch = 0\n        loss = 0\n        if self.is_continue:\n            epoch, model, optimizer, loss = self.__load_checkpoint(model, optimizer, self.checkpoint)\n\n        for training_epoch in range(epoch, epochs):\n            ## change model mode depending on the phase\n            for phase in ['train', 'val']:\n                dataloader = dataloaders[phase]\n                epoch_loss = 0  # Track total loss for the epoch\n                if phase == 'train':\n                    model.train()\n                    for inputs, labels in tqdm(dataloader, desc=phase):\n                        inputs = inputs.to(self.DEVICE)\n                        labels = labels.to(self.DEVICE)\n                        # zero grads of he optim\n                        optimizer.zero_grad()\n                        # freeze the non-learnable weights\n                        self.__handle_transfer_learning(phase, training_epoch / epochs)\n                        # forward pass\n                        logit = model(inputs)\n                        loss = criterion(logit, labels)\n                        loss.backward()\n                        # update weights\n                        optimizer.step()\n                        epoch_loss += loss.item()  # Accumulate loss\n                else:\n                    # skip evaluation if no suitable dataloader\n                    if dataloaders[phase] is None:\n                        continue\n                    model.eval()\n                    self.__eval_model(dataloader)\n            self.__save_checkpoint(training_epoch, model.state_dict(), optimizer.state_dict(), epoch_loss)\n            print(f\"Epoch {training_epoch + 1}/{epochs}, {phase} Loss: {epoch_loss / len(dataloader)}\")  # Print loss\n\n        self.__save_model()\n\n    def __handle_transfer_learning(self, phase, ratio_epochs, tl_coeff=0.8):\n        if phase == \"train\":\n            if ratio_epochs >= tl_coeff:\n                # Unfreeze all layers for fine-tuning\n                for param in self.model.parameters():\n                    param.requires_grad = True\n            else:\n                # Freeze the CNN part\n                for param in self.model.parameters():\n                    param.requires_grad = False\n                # Unfreeze the classification layer\n                for param in self.model.fc.parameters():\n                    param.requires_grad = True\n        elif phase == \"val\":\n            for param in self.model.parameters():\n                param.requires_grad = False\n\n    def __eval_model(self, dataloader):\n        model = self.model\n        criterion = self.criterion\n        model.eval()\n        val_loss = 0\n        correct_preds = 0\n        total_preds = 0\n\n        with torch.no_grad():\n            for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n                inputs = inputs.to(self.DEVICE)\n                labels = labels.to(self.DEVICE)\n                # forward pass\n                logits = model(inputs)\n                loss = criterion(logits, labels)\n                val_loss += loss.item()  # Accumulate loss\n\n                # Compute accuracy\n                _, predicted = torch.max(logits, 1)\n                correct_preds += (predicted == labels).sum().item()\n                total_preds += labels.size(0)\n\n        # Calculate average loss and accuracy\n        avg_loss = val_loss / len(dataloader)\n        accuracy = correct_preds / total_preds\n        return avg_loss, accuracy\n\n    def __save_model(self):\n        torch.save(self.model.state_dict(), self.save_folder + \"/b1_model.pth\")\n\n    def __save_checkpoint(self, epoch, model_state_dict, optimizer_state_dict, loss):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model_state_dict,\n            'optimizer_state_dict': optimizer_state_dict,\n            'loss': loss,\n        }\n        torch.save(checkpoint, self.save_folder + f'checkpoint-epoch{epoch}-loss{loss}.pth')\n\n    def __load_checkpoint(self, model, optimizer, checkpoint_path):\n        checkpoint = torch.load(checkpoint_path)\n        epoch = checkpoint['epoch']\n        model_state_dict = checkpoint['model_state_dict']\n        optimizer_state_dict = checkpoint['optimizer_state_dict']\n        loss = checkpoint['loss']\n        model = model.load_state_dict(model_state_dict)\n        optimizer = optimizer.load_state_dict(optimizer_state_dict)\n        return epoch, model, optimizer, loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:11:00.371126Z","iopub.execute_input":"2025-02-05T22:11:00.371418Z","iopub.status.idle":"2025-02-05T22:11:00.385231Z","shell.execute_reply.started":"2025-02-05T22:11:00.371396Z","shell.execute_reply":"2025-02-05T22:11:00.384386Z"}},"outputs":[],"execution_count":97},{"id":"d2e0e7432bcf15ee","cell_type":"markdown","source":"## Dataset","metadata":{}},{"id":"5fc01b6abf2a455b","cell_type":"code","source":"class B1Dataset(Dataset):\n\n    VIDEO_SPLITS = {\n        'train': {1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54},\n        'val': {0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51},\n        'test': {4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47}\n    }\n\n    def __init__(self, csv_file, split='train', transform=None):\n        self.data = pd.read_csv(csv_file)\n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((256, 256)),\n                transforms.CenterCrop((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n        else:\n            self.transform = transform\n\n        if split in self.VIDEO_SPLITS:\n            self.data = self.data[self.data['video_names'].astype(int).isin(self.VIDEO_SPLITS[split])]\n        else:\n            raise NameError(f'There is no such split: {split}, only {self.VIDEO_SPLITS}')\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path = self.data.iloc[idx]['img_path']\n        label = self.data.iloc[idx]['Mapped_Label']\n\n        # Load image\n        image = Image.open(img_path).convert(\"RGB\")\n\n        # Apply transformations if provided\n        if self.transform:\n            image = self.transform(image)\n        return image, torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:11:42.768975Z","iopub.execute_input":"2025-02-05T22:11:42.769288Z","iopub.status.idle":"2025-02-05T22:11:42.777005Z","shell.execute_reply.started":"2025-02-05T22:11:42.769265Z","shell.execute_reply":"2025-02-05T22:11:42.776190Z"}},"outputs":[],"execution_count":106},{"id":"136b66cec80ecaf6","cell_type":"markdown","source":"## Code","metadata":{}},{"id":"8b5e4d106b988ef4","cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:11:45.324434Z","iopub.execute_input":"2025-02-05T22:11:45.324719Z","iopub.status.idle":"2025-02-05T22:11:45.329859Z","shell.execute_reply.started":"2025-02-05T22:11:45.324697Z","shell.execute_reply":"2025-02-05T22:11:45.329203Z"}},"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":107},{"id":"2ea1f05e86b1d61c","cell_type":"code","source":"train_dataset = B1Dataset(csv_file='/kaggle/working/dataset.csv/dataset.csv', split='train')\nval_dataset = B1Dataset(csv_file='/kaggle/working/dataset.csv/dataset.csv', split='val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:11:46.648880Z","iopub.execute_input":"2025-02-05T22:11:46.649213Z","iopub.status.idle":"2025-02-05T22:11:46.670586Z","shell.execute_reply.started":"2025-02-05T22:11:46.649191Z","shell.execute_reply":"2025-02-05T22:11:46.669978Z"}},"outputs":[],"execution_count":108},{"id":"7b115b6111f9f235","cell_type":"code","source":"batch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\ndataloaders = {'train': train_loader, 'val': val_loader}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:11:48.144470Z","iopub.execute_input":"2025-02-05T22:11:48.144759Z","iopub.status.idle":"2025-02-05T22:11:48.149021Z","shell.execute_reply.started":"2025-02-05T22:11:48.144737Z","shell.execute_reply":"2025-02-05T22:11:48.148201Z"}},"outputs":[],"execution_count":109},{"id":"84b5edf0-20db-4d09-ad38-989254833873","cell_type":"code","source":"!makedir '/kaggle/working/checkpoints/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:15:43.836221Z","iopub.execute_input":"2025-02-05T22:15:43.836509Z","iopub.status.idle":"2025-02-05T22:15:43.979739Z","shell.execute_reply.started":"2025-02-05T22:15:43.836487Z","shell.execute_reply":"2025-02-05T22:15:43.978682Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: makedir: command not found\n","output_type":"stream"}],"execution_count":112},{"id":"b8d7493cca425a71","cell_type":"code","source":"model = ResnetEvolution()\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nsave_folder = '/kaggle/working/'\ntrainer = b1_ModelTrainer(model, optimizer, criterion, epochs=2, dataloaders=dataloaders, device=device, save_folder=save_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:25.516208Z","iopub.execute_input":"2025-02-05T22:16:25.516547Z","iopub.status.idle":"2025-02-05T22:16:26.022546Z","shell.execute_reply.started":"2025-02-05T22:16:25.516517Z","shell.execute_reply":"2025-02-05T22:16:26.021840Z"}},"outputs":[],"execution_count":113},{"id":"1547b20186ca6e47","cell_type":"code","source":"trainer.train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T22:16:28.837446Z","iopub.execute_input":"2025-02-05T22:16:28.837774Z","iopub.status.idle":"2025-02-05T22:18:18.478164Z","shell.execute_reply.started":"2025-02-05T22:16:28.837747Z","shell.execute_reply":"2025-02-05T22:18:18.477361Z"}},"outputs":[{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:34<00:00,  1.95it/s]\nEvaluating: 100%|██████████| 42/42 [00:20<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2, val Loss: 0.0\n","output_type":"stream"},{"name":"stderr","text":"train: 100%|██████████| 68/68 [00:34<00:00,  2.00it/s]\nEvaluating: 100%|██████████| 42/42 [00:19<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/2, val Loss: 0.0\n","output_type":"stream"}],"execution_count":114}]}